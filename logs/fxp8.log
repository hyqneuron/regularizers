Sequential (
  (conv1_1): Sequential (
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (conv1_2): Sequential (
    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  (conv2_1): Sequential (
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (conv2_2): Sequential (
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (pool2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  (conv3_1): Sequential (
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (conv3_2): Sequential (
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (pool3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  (conv4_1): Sequential (
    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (conv4_2): Sequential (
    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (pool4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  (conv5_1): Sequential (
    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (conv5_2): Sequential (
    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (pool5): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  (fc6): Sequential (
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (2): ReLU (inplace)
  )
  (logit): Conv2d(256, 10, kernel_size=(1, 1), stride=(1, 1))
  (flatter): Flatten()
)
{'L1_decay': 5e-06,
 'auto_start': True,
 'batch_size': 128,
 'force_name': False,
 'graphfolder': 'logs/fxp8_graphs/',
 'last2_mult': 1.0,
 'learning_rate': 0.1,
 'logfile': 'logs/fxp8.log',
 'mode': '',
 'momentum': 0.0,
 'name': 'fxp8',
 'num_base': 32,
 'optimizer': 'SGD_var_dup1',
 'weight_decay': 0.00064}
0.00826652180608
0.0926496924936
0.0925432577522
0.184703671883
0.184258851165
0.368501102769
0.370019530153
0.7380243989
0.736385700335
0.738752190383
0.0821184924808
0.0812294928378
Decayed training with schedule: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]
  0   0  50, 2.087337 [1.039708/25.868714] [   0/2.087337]
  0   0 100, 1.981935 [0.195669/17.225754] [   0/1.981935]
  0   0 150, 1.933408 [0.412350/12.932500] [   0/1.933408]
  0   0 200, 1.986979 [0.241247/10.587881] [  50/1.933408]
  0   0 250, 1.989959 [0.107362/9.289237] [ 100/1.933408]
  0   0 300, 2.026912 [0.262460/8.517117] [ 150/1.933408]
  0   0 350, 2.043431 [0.106002/7.993276] [ 200/1.933408]
Showing Monitored Norms
5.938019, 0.535306, 0.477581, 0.240850, 0.211585, 0.136470, 0.139258, 0.096339, 0.100785, 0.147534, 1.086680, 1.471252, 
0.335385, 1.123273, 0.831519, 0.841943, 0.893021, 0.812664, 0.604186, 0.411330, 0.158707, 0.184847, 0.069565, 0.221172, 
0.000000, 0.000019, -0.009963, -0.033866, -0.069105, -0.080451, 0.043680, -0.017828, 0.030179, -0.061614, -0.018723, -0.092895, 
0.049087, 0.049596, 0.044197, 0.044486, 0.038986, 0.050289, 0.051528, 0.071101, 0.074216, 0.108991, 0.089237, 0.119509, 
0.033896, 0.112437, 0.083216, 0.084133, 0.089085, 0.080923, 0.061008, 0.041438, 0.018755, 0.018060, 0.009727, 0.020247, 
Epoch 0 done. Evaluation:
(11444, 50000)
(1307, 10000)
  0   1  50, 2.062431 [0.144497/7.334858] [ 291/1.933408]
  0   1 100, 2.072282 [0.079018/7.031989] [ 341/1.933408]
  0   1 150, 2.085620 [0.054993/6.747696] [ 391/1.933408]
  0   1 200, 2.088512 [0.076074/6.486833] [ 441/1.933408]
  0   1 250, 2.087372 [0.132365/6.245814] [ 491/1.933408]
  0   1 300, 2.098458 [0.096765/6.013955] [ 541/1.933408]
  0   1 350, 2.100774 [0.059480/5.791017] [ 591/1.933408]
Showing Monitored Norms
4.320278, 0.491500, 0.267938, 0.140994, 0.125130, 0.081495, 0.084633, 0.058218, 0.063123, 0.096550, 0.495829, 1.319611, 
0.257333, 0.392003, 0.205848, 0.170300, 0.151294, 0.191782, 0.198331, 0.377838, 0.100923, 0.101526, 0.051442, 0.198081, 
0.000000, -0.000023, -0.006789, -0.001501, -0.005402, -0.003920, 0.003553, -0.020157, -0.043852, -0.091945, -0.050083, -0.144717, 
0.035714, 0.045537, 0.024796, 0.026042, 0.023056, 0.030031, 0.031316, 0.042967, 0.046483, 0.071327, 0.040717, 0.107191, 
0.025980, 0.039464, 0.020652, 0.017205, 0.015223, 0.019351, 0.020134, 0.037799, 0.009094, 0.004774, 0.001502, 0.014036, 
Epoch 1 done. Evaluation:
(9724, 50000)
(1754, 10000)
  0   2  50, 2.102411 [0.167590/5.443283] [ 682/1.933408]
  0   2 100, 2.106600 [0.124122/5.243953] [ 732/1.933408]
  0   2 150, 2.097586 [0.107361/5.069383] [ 782/1.933408]
  0   2 200, 2.088990 [0.086231/4.910624] [ 832/1.933408]
  0   2 250, 2.094649 [0.126655/4.753668] [ 882/1.933408]
  0   2 300, 2.096555 [0.148970/4.598355] [ 932/1.933408]
  0   2 350, 2.091382 [0.143943/4.462530] [ 982/1.933408]
Showing Monitored Norms
3.176817, 0.406081, 0.254802, 0.148156, 0.130118, 0.088702, 0.087356, 0.062182, 0.058795, 0.091484, 0.483941, 1.332225, 
0.176292, 0.430284, 0.638344, 0.541810, 0.420400, 0.169764, 0.095442, 0.085590, 0.061900, 0.064267, 0.038936, 0.161379, 
0.000000, 0.000057, 0.014258, -0.011743, 0.013396, -0.024396, -0.014798, -0.015608, -0.042283, -0.049195, -0.038765, -0.125337, 
0.026261, 0.037623, 0.023580, 0.027365, 0.023975, 0.032687, 0.032323, 0.045892, 0.043296, 0.067584, 0.039741, 0.108216, 
0.017824, 0.043193, 0.063931, 0.054191, 0.042185, 0.016821, 0.009590, 0.008944, 0.004522, 0.004526, 0.000378, 0.010309, 
Epoch 2 done. Evaluation:
(9191, 50000)
(1080, 10000)
  0   3  50, 2.087118 [0.097759/4.230769] [1073/1.933408]
  0   3 100, 2.079327 [0.153442/4.119145] [1123/1.933408]
  0   3 150, 2.083491 [0.101003/4.005834] [1173/1.933408]
  0   3 200, 2.083123 [0.082631/3.892389] [1223/1.933408]
  0   3 250, 2.082981 [0.095218/3.779464] [1273/1.933408]
  0   3 300, 2.085358 [0.084790/3.683316] [1323/1.933408]
  0   3 350, 2.085410 [0.110174/3.585491] [1373/1.933408]
Showing Monitored Norms
2.379639, 0.353017, 0.238074, 0.147228, 0.125758, 0.090717, 0.072370, 0.061741, 0.062746, 0.083117, 0.481707, 1.351451, 
0.477929, 0.515922, 0.168386, 0.177892, 0.244555, 0.201519, 0.341380, 0.302957, 0.171363, 0.183407, 0.036863, 0.173381, 
-0.000001, -0.000006, -0.003115, -0.012956, -0.021728, 0.010989, 0.011292, 0.004607, 0.014311, -0.033031, -0.036168, -0.126817, 
0.019671, 0.032707, 0.022032, 0.027193, 0.023172, 0.033429, 0.026778, 0.045567, 0.046205, 0.061403, 0.039557, 0.109778, 
0.047833, 0.051696, 0.016942, 0.017799, 0.024359, 0.020606, 0.034331, 0.030705, 0.018117, 0.018263, 0.000788, 0.011945, 
Epoch 3 done. Evaluation:
(9216, 50000)
(1000, 10000)
  0   4  50, 2.084121 [0.073063/3.439225] [1464/1.933408]
  0   4 100, 2.080043 [0.132596/3.368230] [1514/1.933408]
  0   4 150, 2.087355 [0.111404/3.285492] [1564/1.933408]
  0   4 200, 2.090042 [0.048457/3.212081] [1614/1.933408]
  0   4 250, 2.098776 [0.175612/3.145869] [1664/1.933408]
  0   4 300, 2.086331 [0.117162/3.094228] [1714/1.933408]
  0   4 350, 2.095982 [0.141095/3.041062] [1764/1.933408]
Showing Monitored Norms
1.868508, 0.376206, 0.218797, 0.164658, 0.142893, 0.089081, 0.089889, 0.075169, 0.072412, 0.076998, 0.474544, 1.335443, 
0.148940, 0.370031, 0.258285, 0.286957, 0.231543, 0.092289, 0.205193, 0.122697, 0.188131, 0.373529, 0.033503, 0.198933, 
-0.000000, 0.000301, -0.008476, -0.017586, -0.015417, 0.015122, 0.017855, 0.010368, 0.056902, -0.003150, -0.032632, -0.128549, 
0.015446, 0.034855, 0.020248, 0.030413, 0.026329, 0.032826, 0.033261, 0.055477, 0.053324, 0.056882, 0.038969, 0.108477, 
0.014974, 0.037170, 0.025841, 0.028670, 0.023129, 0.010290, 0.021071, 0.013886, 0.021049, 0.037736, 0.000988, 0.015314, 
Epoch 4 done. Evaluation:
(9135, 50000)
(1000, 10000)
  0   5  50, 2.086730 [0.067405/2.956803] [1855/1.933408]
  0   5 100, 2.093567 [0.067499/2.912224] [1905/1.933408]
  0   5 150, 2.094930 [0.071906/2.869707] [1955/1.933408]
  0   5 200, 2.086668 [0.146661/2.828464] [2005/1.933408]
  0   5 250, 2.148378 [0.037909/2.712413] [2055/1.933408]
  0   5 300, 2.183050 [0.598763/2.632646] [2105/1.933408]
  0   5 350, 2.154418 [0.086092/2.608003] [2155/1.933408]
Showing Monitored Norms
1.531939, 0.330206, 0.185933, 0.099934, 0.095761, 0.091503, 0.081646, 0.075868, 0.067175, 0.083589, 0.468937, 1.222948, 
0.295955, 0.304983, 0.139165, 0.192007, 0.182970, 0.179295, 0.266238, 0.333709, 0.155934, 0.137988, 0.029508, 0.136824, 
-0.000006, -0.000121, -0.001276, 0.000379, 0.004325, 0.001949, -0.025080, -0.012105, 0.007683, -0.026792, -0.028818, -0.116993, 
0.012664, 0.030594, 0.017207, 0.018458, 0.017645, 0.033719, 0.030211, 0.055993, 0.049467, 0.061752, 0.038508, 0.099339, 
0.029623, 0.030650, 0.014007, 0.019293, 0.018423, 0.018280, 0.026510, 0.033636, 0.016590, 0.013980, 0.001158, 0.007311, 
Epoch 5 done. Evaluation:
(8831, 50000)
(1705, 10000)
  0   6  50, 2.121445 [0.101512/2.578022] [2246/1.933408]
  0   6 100, 2.111590 [0.061915/2.579046] [2296/1.933408]
  0   6 150, 2.100547 [0.180105/2.589508] [2346/1.933408]
  0   6 200, 2.099993 [0.077845/2.602123] [2396/1.933408]
  0   6 250, 2.182029 [0.053732/2.422453] [2446/1.933408]
  0   6 300, 2.171697 [0.051081/2.337970] [2496/1.933408]
  0   6 350, 2.127095 [0.101329/2.391902] [2546/1.933408]
Showing Monitored Norms
1.306098, 0.266606, 0.163885, 0.101728, 0.082574, 0.083321, 0.103756, 0.061233, 0.075193, 0.073506, 0.448966, 1.280459, 
1.303466, 0.756557, 0.608145, 0.409289, 0.108352, 0.087183, 0.095314, 0.099399, 0.053872, 0.087406, 0.027242, 0.126986, 
-0.000006, -0.000537, 0.008621, -0.001454, -0.005476, -0.003119, -0.002506, -0.021056, -0.043786, -0.046011, -0.027031, -0.061153, 
0.010797, 0.024701, 0.015166, 0.018789, 0.015215, 0.030704, 0.038392, 0.045192, 0.055371, 0.054303, 0.036868, 0.104011, 
0.130351, 0.075694, 0.060855, 0.040965, 0.010865, 0.009139, 0.010182, 0.010010, 0.003345, 0.007478, 0.001040, 0.011926, 
Epoch 6 done. Evaluation:
(8798, 50000)
(1001, 10000)
  0   7  50, 2.098064 [0.076920/2.500173] [2637/1.933408]
  0   7 100, 2.164346 [0.143397/2.342677] [2687/1.933408]
  0   7 150, 2.203211 [0.118488/2.161988] [2737/1.933408]
  0   7 200, 2.187596 [0.069674/2.136288] [2787/1.933408]
  0   7 250, 2.179770 [0.111816/2.287347] [2837/1.933408]
  0   7 300, 2.176093 [0.063717/2.186923] [2887/1.933408]
  0   7 350, 2.174894 [0.092840/2.189921] [2937/1.933408]
Showing Monitored Norms
1.208669, 0.230175, 0.175094, 0.153783, 0.057438, 0.043716, 0.056737, 0.040770, 0.051879, 0.074691, 0.407322, 1.114764, 
0.485682, 0.428019, 0.061078, 0.035863, 0.143865, 0.210032, 0.315479, 0.438249, 0.462563, 0.214340, 0.061127, 0.180821, 
0.000005, 0.000215, -0.005273, -0.001956, -0.119394, -0.194470, -0.172010, -0.320278, -0.340238, -0.139281, -0.060543, -0.113410, 
0.009991, 0.021326, 0.016204, 0.028404, 0.010583, 0.016110, 0.020994, 0.030089, 0.038203, 0.055178, 0.033449, 0.090552, 
0.048578, 0.042856, 0.006182, 0.004452, 0.013521, 0.019521, 0.030454, 0.041677, 0.043523, 0.018335, 0.002837, 0.014268, 
Epoch 7 done. Evaluation:
(8707, 50000)
(1194, 10000)
  0   8  50, 2.156519 [0.058688/2.202747] [3028/1.933408]
  0   8 100, 2.143544 [0.066247/2.230781] [3078/1.933408]
  0   8 150, 2.221006 [0.016142/2.022055] [3128/1.933408]
  0   8 200, 2.302937 [0.012795/1.656355] [3178/1.933408]
  0   8 250, 2.302647 [0.010300/1.448035] [3228/1.933408]
  0   8 300, 2.302801 [0.009220/1.319741] [3278/1.933408]
  0   8 350, 2.302526 [0.010981/1.233759] [3328/1.933408]
Showing Monitored Norms
1.022547, 0.030389, 0.018973, 0.002616, 0.000631, 0.000005, 0.000006, 0.000000, 0.000000, 0.000000, 0.044654, 0.132730, 
0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.004663, 
0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, -0.000878, 
0.008453, 0.002816, 0.001756, 0.000483, 0.000116, 0.000002, 0.000002, 0.000000, 0.000000, 0.000000, 0.003667, 0.010782, 
0.000845, 0.000282, 0.000176, 0.000048, 0.000012, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000367, 0.001091, 
Epoch 8 done. Evaluation:
(6310, 50000)
(1000, 10000)
  0   9  50, 2.302723 [0.011842/1.123627] [3419/1.933408]
  0   9 100, 2.302926 [0.008679/1.073844] [3469/1.933408]
  0   9 150, 2.302833 [0.007315/1.028658] [3519/1.933408]
  0   9 200, 2.302731 [0.010593/0.986550] [3569/1.933408]
  0   9 250, 2.302785 [0.009234/0.946460] [3619/1.933408]
  0   9 300, 2.302638 [0.009629/0.908210] [3669/1.933408]
  0   9 350, 2.302814 [0.005915/0.870963] [3719/1.933408]
